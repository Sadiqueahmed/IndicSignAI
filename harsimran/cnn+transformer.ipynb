{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0e8fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "video_dir = r\"C:\\Users\\Harsimran Singh\\Downloads\\WLASL-master\\start_kit\\raw_videos\"\n",
    "output_dir = r\"C:\\Users\\Harsimran Singh\\Downloads\\WLASL-master\\start_kit\\wlasl_frames\"\n",
    "sequence_length = 16\n",
    "\n",
    "with open(r\"C:\\Users\\Harsimran Singh\\Downloads\\WLASL-master\\start_kit\\WLASL_v0.3.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for entry in data:\n",
    "    label = entry[\"gloss\"]\n",
    "    for instance in entry[\"instances\"]:\n",
    "        video_id = instance[\"video_id\"]\n",
    "        video_path = os.path.join(video_dir, video_id + \".mp4\")\n",
    "\n",
    "        if not os.path.exists(video_path):\n",
    "            continue\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "\n",
    "        if len(frames) < sequence_length:\n",
    "            continue\n",
    "\n",
    "        # Sample SEQUENCE_LENGTH evenly spaced frames\n",
    "        step = len(frames) // sequence_length\n",
    "        sampled_frames = [frames[i] for i in range(0, len(frames), step)[:sequence_length]]\n",
    "\n",
    "        label_path = os.path.join(output_dir, label)\n",
    "        os.makedirs(label_path, exist_ok=True)\n",
    "        video_folder = os.path.join(label_path, video_id)\n",
    "        os.makedirs(video_folder, exist_ok=True)\n",
    "\n",
    "        for i, frame in enumerate(sampled_frames):\n",
    "            out_path = os.path.join(video_folder, f\"frame_{i:03d}.jpg\")\n",
    "            cv2.imwrite(out_path, frame)\n",
    "\n",
    "print(\"✅ Frame extraction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1002e78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (Input, TimeDistributed, GlobalAveragePooling1D, Dropout, Dense, MultiHeadAttention, Add, LayerNormalization, Embedding)\n",
    "from tensorflow.keras.applications import EfficientNetV2B0\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter, deque\n",
    "import json\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15d4d031",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "003824c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT, IMG_WIDTH = 64, 64\n",
    "SEQUENCE_LENGTH = 16\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13a30757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_frame(img):\n",
    "    if np.random.rand() < 0.3:\n",
    "        img = cv2.flip(img, 1)\n",
    "    if np.random.rand() < 0.3:\n",
    "        img = cv2.GaussianBlur(img, (3,3), 0)\n",
    "    if np.random.rand() < 0.3:\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "        hsv[...,1] = hsv[...,1] * (0.8 + np.random.rand() * 0.4)\n",
    "        img = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "    if np.random.rand() < 0.3:\n",
    "        brightness = 0.7 + np.random.rand() * 0.6\n",
    "        img = np.clip(img * brightness, 0, 255).astype(np.uint8)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b95f61d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    X, y = [], []\n",
    "    label_list = sorted(os.listdir(path))\n",
    "    label_map = {label: idx for idx, label in enumerate(label_list)}\n",
    "\n",
    "    for label in label_list:\n",
    "        label_path = os.path.join(path, label)\n",
    "        for sample_folder in os.listdir(label_path):\n",
    "            sample_path = os.path.join(label_path, sample_folder)\n",
    "            frames = []\n",
    "            frame_files = sorted(os.listdir(sample_path))[:SEQUENCE_LENGTH]\n",
    "            for frame_name in frame_files:\n",
    "                frame = cv2.imread(os.path.join(sample_path, frame_name))\n",
    "                frame = cv2.resize(frame, (IMG_WIDTH, IMG_HEIGHT))\n",
    "                frame = augment_frame(frame)\n",
    "                frames.append(frame)\n",
    "            if len(frames) == SEQUENCE_LENGTH:\n",
    "                X.append(frames)\n",
    "                y.append(label_map[label])\n",
    "\n",
    "    class_counts = Counter(y)\n",
    "    valid_classes = {cls for cls, count in class_counts.items() if count >= 2}\n",
    "    X_filtered, y_filtered = [], []\n",
    "    for xi, yi in zip(X, y):\n",
    "        if yi in valid_classes:\n",
    "            X_filtered.append(xi)\n",
    "            y_filtered.append(yi)\n",
    "\n",
    "    X = np.array(X_filtered)\n",
    "    X = X.astype(np.float32)\n",
    "    mean = np.mean(X)\n",
    "    std = np.std(X)\n",
    "    X = (X - mean) / (std + 1e-7)\n",
    "\n",
    "    print(f\"✅ Filtered dataset: {len(valid_classes)} classes retained.\")\n",
    "    return X, to_categorical(y_filtered), label_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bc86bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size=64, num_heads=4, ff_dim=128, dropout=0.2):\n",
    "    x = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(inputs, inputs)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Add()([x, inputs])\n",
    "    x = LayerNormalization()(x)\n",
    "\n",
    "    ff = Dense(ff_dim, activation='relu')(x)\n",
    "    ff = Dropout(dropout)(ff)\n",
    "    ff = Dense(inputs.shape[-1])(ff)\n",
    "    x = Add()([x, ff])\n",
    "    return LayerNormalization()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "999625e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_learned_positional_encoding(x):\n",
    "    pos = tf.range(start=0, limit=SEQUENCE_LENGTH, delta=1)\n",
    "    pos_emb = Embedding(input_dim=SEQUENCE_LENGTH, output_dim=x.shape[-1])(pos)\n",
    "    return x + pos_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed7a789d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    base_cnn = EfficientNetV2B0(include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), pooling='avg', weights='imagenet')\n",
    "    base_cnn.trainable = True\n",
    "    x = TimeDistributed(base_cnn)(inputs)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = add_learned_positional_encoding(x)\n",
    "\n",
    "    for _ in range(4):\n",
    "        x = transformer_encoder(x, head_size=128, num_heads=4, ff_dim=256)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    return Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10197ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Filtered dataset: 38 classes retained.\n",
      "Epoch 1/40\n",
      "17/17 [==============================] - 76s 3s/step - loss: 3.9139 - accuracy: 0.0338 - top_k_categorical_accuracy: 0.0977 - val_loss: 3.9003 - val_accuracy: 0.0149 - val_top_k_categorical_accuracy: 0.0597 - lr: 1.0000e-04\n",
      "Epoch 2/40\n",
      "17/17 [==============================] - 46s 3s/step - loss: 3.6212 - accuracy: 0.0602 - top_k_categorical_accuracy: 0.1203 - val_loss: 3.7045 - val_accuracy: 0.0299 - val_top_k_categorical_accuracy: 0.1343 - lr: 1.0000e-04\n",
      "Epoch 3/40\n",
      "17/17 [==============================] - 45s 3s/step - loss: 3.5807 - accuracy: 0.0639 - top_k_categorical_accuracy: 0.1692 - val_loss: 3.7474 - val_accuracy: 0.0299 - val_top_k_categorical_accuracy: 0.1343 - lr: 1.0000e-04\n",
      "Epoch 4/40\n",
      "17/17 [==============================] - 44s 3s/step - loss: 3.4726 - accuracy: 0.0865 - top_k_categorical_accuracy: 0.2105 - val_loss: 3.7414 - val_accuracy: 0.0597 - val_top_k_categorical_accuracy: 0.1045 - lr: 1.0000e-04\n",
      "Epoch 5/40\n",
      "17/17 [==============================] - 45s 3s/step - loss: 3.3517 - accuracy: 0.1316 - top_k_categorical_accuracy: 0.2857 - val_loss: 3.7624 - val_accuracy: 0.0597 - val_top_k_categorical_accuracy: 0.1194 - lr: 1.0000e-04\n",
      "Epoch 6/40\n",
      "17/17 [==============================] - 44s 3s/step - loss: 3.2097 - accuracy: 0.1805 - top_k_categorical_accuracy: 0.3233 - val_loss: 3.7869 - val_accuracy: 0.1343 - val_top_k_categorical_accuracy: 0.2239 - lr: 1.0000e-04\n",
      "Epoch 7/40\n",
      "17/17 [==============================] - 47s 3s/step - loss: 2.9285 - accuracy: 0.2331 - top_k_categorical_accuracy: 0.4436 - val_loss: 3.6938 - val_accuracy: 0.0896 - val_top_k_categorical_accuracy: 0.1940 - lr: 1.0000e-04\n",
      "Epoch 8/40\n",
      "17/17 [==============================] - 47s 3s/step - loss: 2.7941 - accuracy: 0.2519 - top_k_categorical_accuracy: 0.4812 - val_loss: 4.0741 - val_accuracy: 0.0746 - val_top_k_categorical_accuracy: 0.1791 - lr: 1.0000e-04\n",
      "Epoch 9/40\n",
      "17/17 [==============================] - 46s 3s/step - loss: 2.5061 - accuracy: 0.3722 - top_k_categorical_accuracy: 0.5526 - val_loss: 4.0608 - val_accuracy: 0.1791 - val_top_k_categorical_accuracy: 0.2239 - lr: 1.0000e-04\n",
      "Epoch 10/40\n",
      " 3/17 [====>.........................] - ETA: 35s - loss: 2.3360 - accuracy: 0.3542 - top_k_categorical_accuracy: 0.5417"
     ]
    }
   ],
   "source": [
    "X, y, label_map = load_dataset(\"C:/Users/Harsimran Singh/Downloads/WLASL-master/start_kit/wlasl_frames\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "model = build_model((SEQUENCE_LENGTH, IMG_HEIGHT, IMG_WIDTH, 3), len(label_map))\n",
    "model.compile(optimizer=tf.keras.optimizers.AdamW(learning_rate=1e-4, weight_decay=1e-5),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.05),\n",
    "              metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=3)])\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=7, restore_best_weights=True),\n",
    "    LearningRateScheduler(scheduler),\n",
    "    ModelCheckpoint(\"best_model.h5\", save_best_only=True, monitor='val_accuracy', mode='max')\n",
    "]\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "          epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d22b3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(r\"C:\\Users\\Harsimran Singh\\Downloads\\Untitled Folder\\cnn_transformer_sign_model.h5\")\n",
    "with open(\"C:/Users/Harsimran Singh/Downloads/WLASL-master/start_kit/label_map.json\", \"w\") as f:\n",
    "    json.dump({k: int(v) for k, v in label_map.items()}, f)\n",
    "\n",
    "print(\"Training complete and model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d9c245",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\ud83c\\udfa5 Starting real-time prediction. Press 'q' to quit.\")\n",
    "model = load_model(\"C:/Users/Harsimran Singh/Downloads/WLASL-master/start_kit/cnn_transformer_sign_model.h5\")\n",
    "with open(\"C:/Users/Harsimran Singh/Downloads/WLASL-master/start_kit/label_map.json\") as f:\n",
    "    label_map = json.load(f)\n",
    "inv_label_map = {int(v): k for k, v in label_map.items()}\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "def capture_sequence(cap, seq_len=SEQUENCE_LENGTH):\n",
    "    frames = []\n",
    "    for _ in range(seq_len):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        resized = cv2.resize(frame, (IMG_WIDTH, IMG_HEIGHT))\n",
    "        frames.append(resized.astype(np.float32))\n",
    "    X_seq = np.array(frames)\n",
    "    X_seq = (X_seq - np.mean(X_seq)) / (np.std(X_seq) + 1e-7)\n",
    "    return X_seq\n",
    "\n",
    "pred_queue = deque(maxlen=5)\n",
    "\n",
    "while True:\n",
    "    seq = capture_sequence(cap)\n",
    "    if seq.shape[0] != SEQUENCE_LENGTH:\n",
    "        continue\n",
    "\n",
    "    input_tensor = np.expand_dims(seq, axis=0)\n",
    "    preds = model.predict(input_tensor, verbose=0)\n",
    "    label_idx = int(np.argmax(preds))\n",
    "    pred_queue.append(label_idx)\n",
    "\n",
    "    if len(pred_queue) == pred_queue.maxlen:\n",
    "        most_common = Counter(pred_queue).most_common(1)[0][0]\n",
    "        predicted_word = inv_label_map.get(most_common, \"Unknown\")\n",
    "    else:\n",
    "        predicted_word = inv_label_map.get(label_idx, \"Unknown\")\n",
    "\n",
    "    for _ in range(5):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.putText(frame, f\"Prediction: {predicted_word}\", (20, 50),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.imshow(\"Sign Language Predictor\", frame)\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
